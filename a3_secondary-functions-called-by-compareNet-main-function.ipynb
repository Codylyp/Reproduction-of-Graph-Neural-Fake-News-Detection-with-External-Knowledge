{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08551aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "secondary functions called by compareNet main function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0d203e",
   "metadata": {},
   "source": [
    "the model's classifier's class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aab6449",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, params, vocab_size, pte=None):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.params = params\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pte = False if pte is None else True\n",
    "\n",
    "        self.text_encoder = TextEncoder(params)\n",
    "        self.enti_encoder = EntityEncoder(params)\n",
    "        # numOfEntity = 100000\n",
    "        # self.enti_encoder = nn.Embedding(numOfEntity, params.hidden_dim)\n",
    "        # nn.init.xavier_uniform_(self.enti_encoder.weight)\n",
    "        self.topi_encoder = nn.Embedding(100, 100)\n",
    "        self.topi_encoder.from_pretrained(torch.eye(100))\n",
    "        self.match_encoder = MatchingTransform(params)\n",
    "        # self.match_encoder = ConcatTransform(params)   # 参数试验用的\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, params.emb_dim)\n",
    "        if pte is None:\n",
    "            nn.init.xavier_uniform_(self.word_embeddings.weight)\n",
    "        else:\n",
    "            self.word_embeddings.weight.data.copy_(torch.from_numpy(pte))\n",
    "        # KB Field\n",
    "\n",
    "        # with open(self.params.entity_tran, 'rb') as f:\n",
    "        #     transE_embedding = pkl.load(f)\n",
    "        # self.enti_tran = nn.Embedding.from_pretrained(torch.from_numpy(transE_embedding))\n",
    "\n",
    "        self.model = HGAT(params)\n",
    "        self.pooling = Pooling(params)\n",
    "        self.classifier_sen = nn.Linear(params.node_emb_dim, params.ntags)\n",
    "        self.classifier_ent = nn.Linear(params.node_emb_dim, params.ntags)\n",
    "\n",
    "        self.dropout = nn.Dropout(params.dropout, )\n",
    "\n",
    "        # entity_num = transE_embedding.shape[0]\n",
    "        # self.gating = GatingMechanism(params) # 这个要放在最后面，尽量少影响随机初始化\n",
    "\n",
    "    # def forward(self, x_list, adj_list, sentPerDoc, entPerDoc=None):\n",
    "    def forward(self, documents, ent_desc, doc_lens, ent_lens, adj_lists, feature_lists, sentPerDoc, entiPerDoc=None):\n",
    "        x_list = []\n",
    "        embeds_docu = self.word_embeddings(documents)   # sents * max_seq_len * emb\n",
    "        d = self.text_encoder(embeds_docu, doc_lens)    # sents * max_seq_len * hidden\n",
    "        d = self.dropout(F.relu_(d))                     # Relu activation and dropout\n",
    "        x_list.append(d)\n",
    "        if self.params.node_type == 3 or self.params.node_type == 2:\n",
    "            embeds_enti = self.word_embeddings(ent_desc)    # sents * max_seq_len * emb\n",
    "            e = self.enti_encoder(embeds_enti, ent_lens, feature_lists[1])    # sents * max_seq_len * hidden\n",
    "            e = self.dropout(F.relu_(e))                     # Relu activation and dropout\n",
    "            x_list.append(e)\n",
    "        if self.params.node_type == 3 or self.params.node_type == 1:\n",
    "            t = self.topi_encoder(feature_lists[-1])         # tops * hidden\n",
    "            x_list.append(t)\n",
    "\n",
    "        X = self.model(x_list, adj_lists)\n",
    "\n",
    "        X_s = self.pooling(X[0], sentPerDoc)   # 选择句子的部分\n",
    "        output = self.classifier_sen(X_s)\n",
    "\n",
    "        if entiPerDoc is not None:\n",
    "            # E_trans = self.enti_tran(feature_lists[1])\n",
    "            E_GCN = X[1]\n",
    "            # E_KB = self.gating(x_list[1], feature_lists[1])\n",
    "            E_KB = x_list[1]\n",
    "            X_e = self.match_encoder(E_GCN, E_KB)  # 选择实体的部分\n",
    "            X_e = self.pooling(X_e, entiPerDoc)\n",
    "            X_e = self.classifier_ent(X_e)\n",
    "            output += X_e\n",
    "        output = F.softmax(output, dim=1)       # 单分类\n",
    "        # output = torch.sigmoid(output)        # 多分类\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3be9b6a",
   "metadata": {},
   "source": [
    "the submodule (graph neural network)'s class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9dd347",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HGAT(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HGAT, self).__init__()\n",
    "        self.para_init()\n",
    "        self.attention = True\n",
    "        self.lower_attention = True\n",
    "\n",
    "        # self.nonlinear = nn.LeakyReLU(0.2)\n",
    "        self.nonlinear = F.relu_\n",
    "        nfeat_list = [params.hidden_dim] * ({0: 1, 1: 2, 2: 2, 3: 3}[params.node_type])\n",
    "        self.ntype = len(nfeat_list)\n",
    "        nhid = params.node_emb_dim\n",
    "\n",
    "        self.gc2: nn.ModuleList = nn.ModuleList()\n",
    "        if not self.lower_attention:\n",
    "            self.gc1: nn.ModuleList = nn.ModuleList()\n",
    "            for t in range(self.ntype):\n",
    "                self.gc1.append( GraphConvolution(nfeat_list[t], nhid, bias=False) )\n",
    "                self.bias1 = Parameter( torch.FloatTensor(nhid) )\n",
    "                stdv = 1. / math.sqrt(nhid)\n",
    "                self.bias1.data.uniform_(-stdv, stdv)\n",
    "        else:\n",
    "            self.gc1 = GraphAttentionConvolution(nfeat_list, nhid, gamma=0.1)\n",
    "        if self.attention:\n",
    "            self.at1: nn.ModuleList = nn.ModuleList()\n",
    "            for t in range(self.ntype):\n",
    "                self.at1.append( SelfAttention(nhid, t, nhid // 2) )\n",
    "        self.dropout = nn.Dropout(params.dropout)\n",
    "\n",
    "    def para_init(self):\n",
    "        self.attention = False\n",
    "        self.lower_attention = False\n",
    "\n",
    "    def forward(self, x_list, adj_list, adj_all = None):\n",
    "        x0 = x_list\n",
    "\n",
    "        if not self.lower_attention:\n",
    "            x1 = [None for _ in range(self.ntype)]\n",
    "            \n",
    "            for t1 in range(self.ntype):\n",
    "                x_t1 = []\n",
    "                for t2 in range(self.ntype):\n",
    "                    idx = t2\n",
    "                    x_t1.append( self.gc1[idx](x0[t2], adj_list[t1][t2]) + self.bias1 )\n",
    "                if self.attention:\n",
    "                    x_t1, weights = self.at1[t1]( torch.stack(x_t1, dim=1) )\n",
    "                else:\n",
    "                    x_t1 = reduce(torch.add, x_t1)\n",
    "                x_t1 = self.dropout(self.nonlinear(x_t1))\n",
    "                x1[t1] = x_t1\n",
    "        else:\n",
    "            x1 = [None for _ in range(self.ntype)]\n",
    "            x1_in = self.gc1(x0, adj_list)\n",
    "            for t1 in range(len(x1_in)):\n",
    "                x_t1 = x1_in[t1]\n",
    "                if self.attention:\n",
    "                    x_t1, weights = self.at1[t1]( torch.stack(x_t1, dim=1) )\n",
    "                else:\n",
    "                    x_t1 = reduce(torch.add, x_t1)\n",
    "                x_t1 = self.dropout(self.nonlinear(x_t1))\n",
    "                x1[t1] = x_t1\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "    def inference(self, x_list, adj_list, adj_all = None):\n",
    "        return self.forward(x_list, adj_list, adj_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d732899e",
   "metadata": {},
   "source": [
    "encoder, decoder and pooling function, as shown in the model framework picture in report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a56cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TextEncoder(Module):\n",
    "    def __init__(self, params):\n",
    "        super(TextEncoder, self).__init__()\n",
    "\n",
    "        self.lstm = LstmEncoder(params.hidden_dim, params.emb_dim)\n",
    "\n",
    "    def forward(self, embeds, seq_lens):\n",
    "        return self.lstm(embeds, seq_lens)\n",
    "\n",
    "class EntityEncoder(Module):\n",
    "    def __init__(self, params):\n",
    "        super(EntityEncoder, self).__init__()\n",
    "        self.lstm = LstmEncoder(params.hidden_dim, params.emb_dim)\n",
    "        self.gating = GatingMechanism(params)\n",
    "\n",
    "    def forward(self, embeds, seq_lens, Y):\n",
    "        X = self.lstm(embeds, seq_lens)\n",
    "        return self.gating(X, Y)\n",
    "\n",
    "class Pooling(nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(Pooling, self).__init__()\n",
    "        self.mode = params.pooling\n",
    "        self.params = params\n",
    "        if self.mode == 'max':\n",
    "            self.pooling = torch.max\n",
    "        elif self.mode == 'sum':\n",
    "            self.pooling = torch.sum\n",
    "        elif self.mode == 'mean':\n",
    "            self.pooling = torch.mean\n",
    "        elif self.mode == 'att':\n",
    "            self.pooling = AttentionPooling(self.params)\n",
    "        else:\n",
    "            raise Exception(\"Unknown pooling mode: {}. (Supported: max, sum, mean, att)\".format(self.mode))\n",
    "\n",
    "    def forward(self, X, sentPerDoc):\n",
    "        '''\n",
    "        :param X:           A tensor with shape:  (D1 + D2 + ... + Dn) * H\n",
    "        :param sentPerDoc:  A tensor with values: [D1, D2, ..., Dn]\n",
    "        :return:            A tensor with shape:  n * H\n",
    "        '''\n",
    "        # weight = [torch.ones((1, i.item()), device=sentPerDoc.device) for i in sentPerDoc]\n",
    "        # weight = block_diag([m.to_sparse() for m in weight]).to_dense()\n",
    "        sentPerDoc = sentPerDoc.cpu().numpy().tolist()\n",
    "        sents = [X[sum(sentPerDoc[: i]): sum(sentPerDoc[: i+1])] for i in range(len(sentPerDoc))]\n",
    "        output = []\n",
    "        for s in sents:\n",
    "            if s.shape[0] == 0:\n",
    "                output.append(torch.zeros((1, s.shape[1]), device=s.device, dtype=X.dtype))\n",
    "            else:\n",
    "                cache = self.pooling(s, dim=0, keepdim=True)\n",
    "                output.append(cache[0] if isinstance(cache, tuple) else cache)\n",
    "        output = torch.cat(output, dim=0)\n",
    "        return output\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
